{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f2f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/satyaprk/Documents/intern_codes/robustness_vs_counterfactuals/\")\n",
    "sys.path.append(\"/Users/skrishna/Documents/phd_codes/neurips_paper/\")\n",
    "sys.path.append(\"/Users/satyaprk/Documents/intern_codes/robustness_vs_counterfactuals/Recourse_Methods/AR\")\n",
    "sys.path.append(\"/Users/satyaprk/Documents/intern_codes/robustness_vs_counterfactuals/Recourse_Methods/Generative_Model\")\n",
    "\n",
    "\n",
    "sys.path.append(\"/Users/satyaprk/Documents/intern_codes/robustness_vs_counterfactuals/Recourse_Methods/AR/recourse/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d37726ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import pickle as pkl\n",
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "import ML_Models.data_loader as loader\n",
    "# from utils import _get_input_subset\n",
    "# from Recourse_Methods.gradient_methods import SCFE\n",
    "# from utils import get_recourses, get_performance_measures\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071b0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset Prep\n",
    "\n",
    "\n",
    "from torchvision import  datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# data_name = \"compas\"\n",
    "data_name = \"adult\"\n",
    "# data_name = \"german\"\n",
    "n_starting_instances = 1200\n",
    "compas_dict = {\n",
    "        \"data_path\": '../Data_Sets/COMPAS/',\n",
    "        \"filename_train\": 'compas-train.csv',\n",
    "        \"filename_test\": 'compas-test.csv',\n",
    "        \"label\": \"risk\",\n",
    "        \"task\": \"classification\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"d\": 6,\n",
    "        \"H1\": 10,\n",
    "        \"H2\": 10,\n",
    "        \"activFun\": nn.Softplus(),\n",
    "        \"n_starting_instances\": n_starting_instances\n",
    "    }\n",
    "\n",
    "german_dict = {\n",
    "        \"data_path\": '../Data_Sets/German_Credit_Data/',\n",
    "        \"filename_train\": 'german-train.csv',\n",
    "        \"filename_test\": 'german-test.csv',\n",
    "        \"label\": \"credit-risk\",\n",
    "        \"task\": \"classification\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"d\": 6,\n",
    "        \"H1\": 10,\n",
    "        \"H2\": 10,\n",
    "    }\n",
    "\n",
    "adult_dict = {\n",
    "        \"data_path\": \"../Data_Sets/Adult/\",\n",
    "        \"filename_train\": 'adult-train.csv',\n",
    "        \"filename_test\": 'adult-test.csv',\n",
    "        \"label\": 'income',\n",
    "        \"task\": \"classification\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"d\": 6,\n",
    "        \"H1\": 25,\n",
    "        \"H2\": 25,\n",
    "        \"activFun\": nn.Softplus(),\n",
    "        \"n_starting_instances\": n_starting_instances\n",
    "    }\n",
    "\n",
    "\n",
    "data_meta_dictionaries = {\n",
    "        \"compas\": compas_dict, \n",
    "        \"adult\": adult_dict, \n",
    "        \"german\":german_dict\n",
    "    }\n",
    "data_meta_info = data_meta_dictionaries[data_name]\n",
    "\n",
    "\n",
    "dataset_test = loader.DataLoader_Tabular(path=data_meta_info[\"data_path\"],\n",
    "                                                 filename=data_meta_info[\"filename_test\"],\n",
    "                                                 label=data_meta_info[\"label\"])\n",
    "        \n",
    "dataset_train = loader.DataLoader_Tabular(path=data_meta_info[\"data_path\"],\n",
    "                                                  filename=data_meta_info[\"filename_train\"],\n",
    "                                                  label=data_meta_info[\"label\"])\n",
    "\n",
    "\n",
    "column_names = pd.read_csv(data_meta_info[\"data_path\"] + data_meta_info[\"filename_train\"]).drop(data_meta_info[\"label\"], axis=1).columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9aca8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size = 32, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size = 32, shuffle=False)\n",
    "\n",
    "data = [i for i in train_loader]\n",
    "num_input = len(data[0][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268daae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Err\tTrain Loss\tTest Err\tTest Loss\n",
      "0.212925\t0.429760\t0.209619\t0.416164\n",
      "0.193687\t0.401795\t0.199558\t0.403731\n",
      "0.185173\t0.391062\t0.186291\t0.393551\n",
      "0.179230\t0.383375\t0.181205\t0.387143\n",
      "0.175249\t0.377968\t0.180431\t0.382125\n",
      "0.175028\t0.374016\t0.178773\t0.378552\n",
      "0.173563\t0.370759\t0.176893\t0.375831\n",
      "0.174089\t0.368428\t0.178662\t0.373293\n",
      "0.172485\t0.366408\t0.178109\t0.371013\n",
      "0.172154\t0.364557\t0.178331\t0.369768\n",
      "0.171960\t0.363136\t0.178994\t0.369163\n",
      "0.171739\t0.361900\t0.177999\t0.367320\n",
      "0.170937\t0.361003\t0.179215\t0.368446\n",
      "0.170799\t0.360033\t0.178883\t0.367251\n",
      "0.169721\t0.359289\t0.176562\t0.364474\n",
      "0.170025\t0.358738\t0.175567\t0.363303\n",
      "0.170910\t0.357948\t0.175124\t0.363245\n",
      "0.169776\t0.357450\t0.175677\t0.362201\n",
      "0.169721\t0.356891\t0.174240\t0.361668\n",
      "0.169196\t0.356572\t0.173908\t0.361554\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# do a single pass over the data\n",
    "def epoch(loader, model, opt=None):\n",
    "    total_loss, total_err = 0.,0.\n",
    "    for X,y, ind in loader:\n",
    "        yp = model(X.view(X.shape[0], -1).to(torch.float32))[:,0]\n",
    "        loss = nn.BCEWithLogitsLoss()(yp, y.float())\n",
    "        if opt:\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        total_err += ((yp > 0) * (y==0) + (yp < 0) * (y==1)).sum().item()\n",
    "        total_loss += loss.item() * X.shape[0]\n",
    "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
    "\n",
    "model = nn.Linear(num_input, 1)\n",
    "opt = optim.SGD(model.parameters(), lr=1e-1)\n",
    "print(\"Train Err\", \"Train Loss\", \"Test Err\", \"Test Loss\", sep=\"\\t\")\n",
    "for i in range(20):\n",
    "    train_err, train_loss = epoch(train_loader, model, opt)\n",
    "    test_err, test_loss = epoch(test_loader, model)\n",
    "    print(*(\"{:.6f}\".format(i) for i in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73adafdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9407407407407408, 2.790132334431342)\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.2\n",
    "delta = epsilon * model.weight.detach().sign()\n",
    "\n",
    "# Testing error on adversarial sample \n",
    "def epoch_adv(loader, model, delta):\n",
    "    total_loss, total_err = 0.,0.\n",
    "    for X,y,ind in loader:\n",
    "        X = X.to(torch.float32)\n",
    "#         tmp = ((2*y.float()-1)[:, None]*delta.squeeze(0)).squeeze(0)\n",
    "        yp = model(X-((2*y.float()-1)[:, None]*delta.squeeze(0)).squeeze(0)).squeeze(-1)\n",
    "#         print(yp)\n",
    "        loss = nn.BCEWithLogitsLoss()(yp, y.float())\n",
    "        total_err += ((yp > 0) * (y==0) + (yp < 0) * (y==1)).sum().item()\n",
    "        total_loss += loss.item() * X.shape[0]\n",
    "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
    "print(epoch_adv(test_loader, model, delta[None,None,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9d5def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rob. Train Err\tRob. Train Loss\tRob. Test Err\tRob. Test Loss\n",
      "0.249827\t0.560851\t0.247872\t0.563151\n",
      "0.247837\t0.558472\t0.247872\t0.557751\n",
      "0.247837\t0.558278\t0.247872\t0.558047\n",
      "0.247837\t0.558109\t0.247872\t0.560323\n",
      "0.247837\t0.558191\t0.247872\t0.557862\n",
      "0.247837\t0.558153\t0.247872\t0.558181\n",
      "0.247837\t0.558303\t0.247872\t0.560162\n",
      "0.247837\t0.558216\t0.247872\t0.558907\n",
      "0.247837\t0.558248\t0.247872\t0.559119\n",
      "0.247837\t0.558192\t0.247872\t0.557164\n",
      "0.247837\t0.558168\t0.247872\t0.559656\n",
      "0.247837\t0.558373\t0.247872\t0.556316\n",
      "0.247837\t0.558212\t0.247872\t0.560921\n",
      "0.247837\t0.558270\t0.247872\t0.560948\n",
      "0.247837\t0.558251\t0.247872\t0.558048\n",
      "0.247837\t0.558340\t0.247872\t0.557774\n",
      "0.247837\t0.558226\t0.247872\t0.558286\n",
      "0.247837\t0.558220\t0.247872\t0.558997\n",
      "0.247837\t0.558193\t0.247872\t0.557808\n",
      "0.247837\t0.558211\t0.247872\t0.564039\n"
     ]
    }
   ],
   "source": [
    "def epoch_robust(loader, model, epsilon, opt=None):\n",
    "    total_loss, total_err = 0.,0.\n",
    "    for X,y,ind in loader:\n",
    "        yp = model(X.view(X.shape[0], -1).to(torch.float32))[:,0] - epsilon*(2*y.float()-1)*model.weight.norm(1)\n",
    "        loss = nn.BCEWithLogitsLoss()(yp, y.float())\n",
    "        if opt:\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        total_err += ((yp > 0) * (y==0) + (yp < 0) * (y==1)).sum().item()\n",
    "        total_loss += loss.item() * X.shape[0]\n",
    "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "model_robust = nn.Linear(num_input, 1)\n",
    "opt = optim.SGD(model_robust.parameters(), lr=1e-1)\n",
    "epsilon = 0.2\n",
    "print(\"Rob. Train Err\", \"Rob. Train Loss\", \"Rob. Test Err\", \"Rob. Test Loss\", sep=\"\\t\")\n",
    "for i in range(20):\n",
    "    train_err, train_loss = epoch_robust(train_loader, model_robust, epsilon, opt)\n",
    "    test_err, test_loss = epoch_robust(test_loader, model_robust, epsilon)\n",
    "    print(*(\"{:.6f}\".format(i) for i in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102c53ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recourse Method -1 \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import datetime\n",
    "\n",
    "\n",
    "class SCFE:\n",
    "    \n",
    "    def __init__(self, classifier, target_threshold: float = 0, _lambda: float = 10.0,\n",
    "                 lr: float = 0.1, max_iter: int = 500, t_max_min: float = 0.5,\n",
    "                 step: float = 0.10, norm: int = 1, optimizer: str = 'adam'):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.model_classification = classifier\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.optimizer = optimizer\n",
    "        self.t_max_min = t_max_min\n",
    "        self.norm = norm\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.target_thres = target_threshold\n",
    "        self._lambda = _lambda\n",
    "        self.step = step\n",
    "    \n",
    "    def generate_counterfactuals(self, query_instance: torch.tensor, target_class: int = 1) -> torch.tensor:\n",
    "        \"\"\"\n",
    "            query instance: the point to be explained\n",
    "            target_class: Direction of the desired change. If target_class = 1, we aim to improve the score,\n",
    "                if target_class = 0, we aim to decrese it (in classification and regression problems).\n",
    "            _lambda: Lambda parameter (distance regularization) parameter of the problem\n",
    "        \"\"\"\n",
    "        \n",
    "        if target_class == 1:\n",
    "            target_prediction = torch.tensor(1).float()\n",
    "        else:\n",
    "            target_prediction = torch.tensor(0).float()\n",
    "        \n",
    "        output = self._call_model(query_instance.reshape(1, -1))\n",
    "        output = self._call_model(query_instance)\n",
    "        \n",
    "        cf = query_instance.clone().requires_grad_(True)\n",
    "        \n",
    "        if self.optimizer == 'adam':\n",
    "            optim = torch.optim.Adam([cf], self.lr)\n",
    "        else:\n",
    "            optim = torch.optim.RMSprop([cf], self.lr)\n",
    "        \n",
    "        # Timer\n",
    "        t0 = datetime.datetime.now()\n",
    "        t_max = datetime.timedelta(minutes=self.t_max_min)\n",
    "        \n",
    "        counterfactuals = []\n",
    "        while not self._check_cf_valid(output, target_class):\n",
    "#             print(\"in...\")\n",
    "            iter = 0\n",
    "            distances = []\n",
    "            all_loss = []\n",
    "            while not self._check_cf_valid(output, target_class) and iter < self.max_iter:\n",
    "                cf.requires_grad = True\n",
    "                total_loss, loss_distance = self.compute_loss(self._lambda, cf,\n",
    "                                                              query_instance,\n",
    "                                                              target_prediction)\n",
    "                optim.zero_grad()\n",
    "                total_loss.backward(retain_graph=True)\n",
    "                optim.step()\n",
    "                \n",
    "                output = self._call_model(cf)\n",
    "                \n",
    "                if self._check_cf_valid(output, target_class):\n",
    "                    counterfactuals.append(cf.detach())\n",
    "                    distances.append(loss_distance.clone().detach())\n",
    "                    all_loss.append(total_loss.detach())\n",
    "                \n",
    "                iter = iter + 1\n",
    "            \n",
    "            output = self._call_model(cf).reshape(1, -1).detach()\n",
    "            if datetime.datetime.now() - t0 > t_max:\n",
    "                break\n",
    "\n",
    "            if self.step == 0.0:  # Don't search over lambdas\n",
    "                break\n",
    "            else:\n",
    "                self._lambda -= self.step\n",
    "\n",
    "        if not len(counterfactuals):\n",
    "            print('No CE found')\n",
    "            cf.detach_()\n",
    "            return -1\n",
    "        \n",
    "        # Choose the nearest counterfactual\n",
    "        counterfactuals = torch.stack(counterfactuals)\n",
    "        distances = torch.stack(distances)\n",
    "        distances = distances.detach()\n",
    "        index = torch.argmin(distances)\n",
    "        counterfactuals = counterfactuals.detach()\n",
    "\n",
    "        ce_star = counterfactuals[index]\n",
    "        distance_star = distances[index]\n",
    "        \n",
    "        \n",
    "        return ce_star, distance_star\n",
    "#         return ce_star\n",
    "\n",
    "    \n",
    "    def compute_loss(self, _lambda: float, cf_candidate: torch.tensor, original_instance: torch.tensor,\n",
    "                     target: torch.tensor) -> torch.tensor:\n",
    "        output = self._call_model(cf_candidate)\n",
    "        # classification loss\n",
    "        bce_loss = nn.BCEWithLogitsLoss()\n",
    "#         print(\"Testing code : \" , output, target)\n",
    "        loss_classification = bce_loss(output, target)\n",
    "        # distance loss\n",
    "        loss_distance = torch.norm((cf_candidate - original_instance), self.norm)\n",
    "        # full loss\n",
    "        total_loss = loss_classification + _lambda * loss_distance\n",
    "        return total_loss, loss_distance\n",
    "\n",
    "    def _call_model(self, cf_candidate):\n",
    "        output = self.model_classification(cf_candidate)[0]\n",
    "#         print(output)\n",
    "        return output\n",
    "\n",
    "    def _check_cf_valid(self, output, target_class):\n",
    "        \"\"\" Check if the output constitutes a sufficient CF-example.\n",
    "            target_class = 1 in general means that we aim to improve the score,\n",
    "            whereas for target_class = 0 we aim to decrese it.\n",
    "        \"\"\"\n",
    "        if target_class == 1:\n",
    "            check = output >= self.target_thres\n",
    "            return check\n",
    "        else:\n",
    "            check = output <= self.target_thres\n",
    "            return check\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2edb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recourse Method 2\n",
    "import Recourse_Methods.Generative_Model.model as model_vae\n",
    "from numpy import linalg as LA\n",
    "\n",
    "# Second class of counter-factual explanation methods         \n",
    "class CCHVAE:\n",
    "\n",
    "    def __init__(self, classifier, model_vae, target_threshold: float = 0,\n",
    "                 n_search_samples: int = 10000, p_norm: int = 1,\n",
    "                 step: float = 0.05, max_iter: int = 1000, clamp: bool = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "        self.generative_model = model_vae\n",
    "        self.n_search_samples = n_search_samples\n",
    "        self.p_norm = p_norm\n",
    "        self.step = step\n",
    "        self.max_iter = max_iter\n",
    "        self.clamp = clamp\n",
    "        self.target_treshold = target_threshold\n",
    "\n",
    "    def hyper_sphere_coordindates(self, instance, high, low):\n",
    "    \n",
    "        \"\"\"\n",
    "        :param n_search_samples: int > 0\n",
    "        :param instance: numpy input point array\n",
    "        :param high: float>= 0, h>l; upper bound\n",
    "        :param low: float>= 0, l<h; lower bound\n",
    "        :param p: float>= 1; norm\n",
    "        :return: candidate counterfactuals & distances\n",
    "        \"\"\"\n",
    "    \n",
    "        delta_instance = np.random.randn(self.n_search_samples, instance.shape[1])\n",
    "        dist = np.random.rand(self.n_search_samples) * (high - low) + low  # length range [l, h)\n",
    "        norm_p = LA.norm(delta_instance, ord=self.p_norm, axis=1)\n",
    "        d_norm = np.divide(dist, norm_p).reshape(-1, 1)  # rescale/normalize factor\n",
    "        delta_instance = np.multiply(delta_instance, d_norm)\n",
    "        candidate_counterfactuals = instance + delta_instance\n",
    "    \n",
    "        return candidate_counterfactuals, dist\n",
    "\n",
    "    def generate_counterfactuals(self, query_instance: torch.tensor, target_class: int = 1) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        :param instance: np array\n",
    "        :return: best CE\n",
    "        \"\"\"  #\n",
    "\n",
    "        # init step size for growing the sphere\n",
    "        low = 0\n",
    "        high = low + self.step\n",
    "\n",
    "        # counter\n",
    "        count = 0\n",
    "        counter_step = 1\n",
    "        query_instance = query_instance.detach().numpy()\n",
    "\n",
    "        # get predicted label of instance\n",
    "        self.classifier.eval()\n",
    "        instance_label = 1 - target_class\n",
    "        # vectorize z\n",
    "        z = self.generative_model.encode_csearch(torch.from_numpy(query_instance).float()).detach().numpy()\n",
    "        z_rep = np.repeat(z.reshape(1, -1), self.n_search_samples, axis=0)\n",
    "\n",
    "        while True:\n",
    "            count = count + counter_step\n",
    "            if count > self.max_iter:\n",
    "                candidate_counterfactual_star = np.empty(query_instance.shape[0], )\n",
    "                candidate_counterfactual_star[:] = np.nan\n",
    "                distance_star = -1\n",
    "                print('No CE found')\n",
    "                break\n",
    "\n",
    "            # STEP 1 -- SAMPLE POINTS on hypersphere around instance\n",
    "            latent_neighbourhood, _ = CCHVAE.hyper_sphere_coordindates(self, z_rep, high, low)\n",
    "            \n",
    "#             print(\"-->> \", latent_neighbourhood)\n",
    "            x_ce = self.generative_model.decode_csearch(torch.from_numpy(latent_neighbourhood).float()).detach().numpy()\n",
    "            \n",
    "            \n",
    "            # why do we have this?\n",
    "#             print(x_ce)\n",
    "            if self.clamp:\n",
    "                x_ce = x_ce.clip(-4, 4)\n",
    "\n",
    "            # STEP 2 -- COMPUTE l1 & l2 norms\n",
    "            if self.p_norm == 1:\n",
    "                distances = np.abs((x_ce - query_instance)).sum(axis=1)\n",
    "            elif self.p_norm == 2:\n",
    "                distances = LA.norm(x_ce - query_instance, axis=1)\n",
    "            else:\n",
    "                print('Distance not defined yet')\n",
    "            \n",
    "            # counterfactual labels\n",
    "#             print(self.classifier(torch.from_numpy(x_ce).float()).detach().numpy())\n",
    "            y_candidate = torch.stack([torch.tensor([int(i[0])]) for i in self.classifier(torch.from_numpy(x_ce).float()).detach().numpy() > 0])\n",
    "            #print(\"Y_Cands : \", self.classifier(torch.from_numpy(x_ce).float()).detach().numpy())\n",
    "            indeces = np.where(y_candidate != instance_label)[0]\n",
    "#             print(\"Indeces : \", indeces)\n",
    "            candidate_counterfactuals = x_ce[indeces]\n",
    "            candidate_dist = distances[indeces]\n",
    "            \n",
    "            if len(candidate_dist) == 0:  # no candidate found & push search range outside\n",
    "                low = high\n",
    "                high = low + self.step\n",
    "            elif len(candidate_dist) > 0:  # certain candidates generated\n",
    "                min_index = np.argmin(candidate_dist)\n",
    "                candidate_counterfactual_star = candidate_counterfactuals[min_index]\n",
    "                distance_star = np.abs(candidate_counterfactual_star - query_instance).sum()\n",
    "                break\n",
    "\n",
    "        return torch.tensor(candidate_counterfactual_star), torch.tensor(distance_star)\n",
    "#         return  torch.tensor(distance_star)     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b6513ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_input_subset(model_sub, inputs: torch.tensor,\n",
    "                      subset_size: int = 100,\n",
    "                      decision_threshold: float = 0) -> torch.tensor:\n",
    "    \n",
    "    \"\"\"\n",
    "    Get negatively classified inputs & return their predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    yhat = (model_sub(inputs) > decision_threshold) * 1\n",
    "    check = (model_sub(inputs) < decision_threshold).detach().numpy()\n",
    "    selected_indices = np.where(check)[0]\n",
    "    input_subset = inputs[selected_indices]\n",
    "    predicted_label_subset = yhat[selected_indices]\n",
    "    return input_subset, predicted_label_subset #[0:subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06dcc53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gotten 0 done...\n",
      "Gotten 100 done...\n",
      "Gotten 200 done...\n",
      "Gotten 300 done...\n",
      "Gotten 400 done...\n",
      "Gotten 500 done...\n",
      "Gotten 600 done...\n",
      "Gotten 700 done...\n",
      "Gotten 800 done...\n",
      "Gotten 900 done...\n"
     ]
    }
   ],
   "source": [
    "# scfe = SCFE(classifier=model, lr=1e-2, _lambda=0.10, step=0.00, max_iter=40000, target_threshold=0)\n",
    "# scfe_r = SCFE(classifier=model_robust, lr=1e-2, _lambda=0.10, step=0.00, max_iter=40000, target_threshold=0)\n",
    "# f(x) = sign(wTX + b)\n",
    "scfe = SCFE(classifier=model, lr=1e-3, _lambda=0.00, step=0.00, max_iter=10000, target_threshold=0)\n",
    "scfe_r = SCFE(classifier=model_robust, lr=1e-3, _lambda=0.00, step=0.00, max_iter=10000, target_threshold=0)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "subset_size = 200\n",
    "testloader = DataLoader(dataset_test,\n",
    "                        batch_size=data_meta_info[\"n_starting_instances\"],\n",
    "                        shuffle=False)\n",
    "\n",
    "data_iter = iter(testloader)\n",
    "inputs, labels, indeces = data_iter.next()\n",
    "inputs = inputs.to(device).float()\n",
    "inputs, predicted_classes = _get_input_subset(model, inputs, subset_size, decision_threshold=0)\n",
    "\n",
    "cfe_scfe = []\n",
    "for i in range(len(inputs)):\n",
    "    if i%100 == 0:\n",
    "        print(\"Gotten {} done...\".format(i))\n",
    "    cfe, ctr_val = scfe.generate_counterfactuals(\n",
    "                    query_instance=inputs[i],\n",
    "                    target_class=1)\n",
    "    if ctr_val.item() > 0:\n",
    "        cfe_scfe.append(cfe)\n",
    "        \n",
    "# distance_scfe = [scfe.generate_counterfactuals(\n",
    "#                     query_instance=inputs[i],\n",
    "#                     target_class=1).item() for i in range(len(inputs)) if scfe.generate_counterfactuals(\n",
    "#                     query_instance=inputs[i],\n",
    "#                     target_class=1) > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54b0e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VAEs\n",
    "\n",
    "vae_path = \"../Recourse_Methods/Generative_Model/Saved_Models/\"\n",
    "\n",
    "input_size = dataset_train.get_number_of_features()\n",
    "\n",
    "vae_model = model_vae.VAE_model(input_size,\n",
    "                                data_meta_info['activFun'],\n",
    "                                data_meta_info['d'],\n",
    "                                data_meta_info['H1'],\n",
    "                                data_meta_info['H2'])\n",
    "\n",
    "data_meta_info[\"vae_path\"] = vae_path + f\"vae_{data_name}.pt\"\n",
    "# print(vae_path + f\"vae_{data_name}.pt\")\n",
    "\n",
    "vae_model.load_state_dict(torch.load(data_meta_info[\"vae_path\"]))\n",
    "        \n",
    "testloader = DataLoader(dataset_test,\n",
    "                        batch_size=data_meta_info[\"n_starting_instances\"],\n",
    "                        shuffle=True)\n",
    "        \n",
    "trainloader = DataLoader(dataset_train,\n",
    "                         batch_size=data_meta_info[\"n_starting_instances\"],\n",
    "                         shuffle=True)\n",
    "\n",
    "\n",
    "# This method makes the \"Manifold assumption\" and uses random search in latent space\n",
    "cchvae = CCHVAE(classifier=model, model_vae=vae_model, step=4.5, max_iter=10000, target_threshold=0)\n",
    "cchvae_r = CCHVAE(classifier=model_robust, model_vae=vae_model, step=4.5, max_iter=10000, target_threshold=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7203af6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200, 13])\n",
      "tensor([[0.1370, 0.1511, 0.5333, 0.0000, 0.0000, 0.1939, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 0.0000, 1.0000]])\n",
      "Gotten 0 done...\n",
      "Gotten 10 done...\n",
      "Gotten 20 done...\n",
      "Gotten 30 done...\n",
      "Gotten 40 done...\n",
      "Gotten 50 done...\n",
      "Gotten 60 done...\n",
      "Gotten 70 done...\n",
      "Gotten 80 done...\n",
      "Gotten 90 done...\n",
      "Gotten 100 done...\n",
      "Gotten 110 done...\n",
      "Gotten 120 done...\n",
      "Gotten 130 done...\n",
      "Gotten 140 done...\n",
      "Gotten 150 done...\n",
      "Gotten 160 done...\n",
      "Gotten 170 done...\n",
      "Gotten 180 done...\n",
      "Gotten 190 done...\n",
      "Gotten 200 done...\n",
      "Gotten 210 done...\n",
      "Gotten 220 done...\n",
      "Gotten 230 done...\n",
      "Gotten 240 done...\n",
      "Gotten 250 done...\n",
      "Gotten 260 done...\n",
      "Gotten 270 done...\n",
      "Gotten 280 done...\n",
      "Gotten 290 done...\n",
      "Gotten 300 done...\n",
      "Gotten 310 done...\n",
      "Gotten 320 done...\n",
      "Gotten 330 done...\n",
      "Gotten 340 done...\n",
      "Gotten 350 done...\n",
      "Gotten 360 done...\n",
      "Gotten 370 done...\n",
      "Gotten 380 done...\n",
      "Gotten 390 done...\n",
      "Gotten 400 done...\n",
      "Gotten 410 done...\n",
      "Gotten 420 done...\n",
      "Gotten 430 done...\n",
      "Gotten 440 done...\n",
      "Gotten 450 done...\n",
      "Gotten 460 done...\n",
      "Gotten 470 done...\n",
      "Gotten 480 done...\n",
      "Gotten 490 done...\n",
      "Gotten 500 done...\n",
      "Gotten 510 done...\n",
      "Gotten 520 done...\n",
      "Gotten 530 done...\n",
      "Gotten 540 done...\n",
      "Gotten 550 done...\n",
      "Gotten 560 done...\n",
      "Gotten 570 done...\n",
      "Gotten 580 done...\n",
      "Gotten 590 done...\n",
      "Gotten 600 done...\n",
      "Gotten 610 done...\n",
      "Gotten 620 done...\n",
      "Gotten 630 done...\n",
      "Gotten 640 done...\n",
      "Gotten 650 done...\n",
      "Gotten 660 done...\n",
      "Gotten 670 done...\n",
      "Gotten 680 done...\n",
      "Gotten 690 done...\n",
      "Gotten 700 done...\n",
      "Gotten 710 done...\n",
      "Gotten 720 done...\n",
      "Gotten 730 done...\n",
      "Gotten 740 done...\n",
      "Gotten 750 done...\n",
      "Gotten 760 done...\n",
      "Gotten 770 done...\n",
      "Gotten 780 done...\n",
      "Gotten 790 done...\n",
      "Gotten 800 done...\n",
      "Gotten 810 done...\n",
      "Gotten 820 done...\n",
      "Gotten 830 done...\n",
      "Gotten 840 done...\n",
      "Gotten 850 done...\n",
      "Gotten 860 done...\n",
      "Gotten 870 done...\n",
      "Gotten 880 done...\n",
      "Gotten 890 done...\n",
      "Gotten 900 done...\n",
      "Gotten 910 done...\n",
      "Gotten 920 done...\n",
      "Gotten 930 done...\n",
      "Gotten 940 done...\n",
      "Gotten 950 done...\n",
      "Gotten 960 done...\n",
      "Gotten 970 done...\n",
      "Gotten 980 done...\n",
      "Gotten 990 done...\n",
      "Gotten 1000 done...\n",
      "Gotten 1010 done...\n",
      "Gotten 1020 done...\n",
      "Gotten 1030 done...\n",
      "Gotten 1040 done...\n",
      "Gotten 1050 done...\n",
      "Gotten 1060 done...\n",
      "Gotten 1070 done...\n",
      "Gotten 1080 done...\n",
      "Gotten 1090 done...\n",
      "Gotten 1100 done...\n",
      "Gotten 1110 done...\n",
      "Gotten 1120 done...\n",
      "Gotten 1130 done...\n",
      "Gotten 1140 done...\n",
      "Gotten 1150 done...\n",
      "Gotten 1160 done...\n",
      "Gotten 1170 done...\n",
      "Gotten 1180 done...\n",
      "Gotten 1190 done...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "subset_size = 1\n",
    "testloader = DataLoader(dataset_test,\n",
    "                        batch_size=data_meta_info[\"n_starting_instances\"],\n",
    "                        shuffle=False)\n",
    "\n",
    "data_iter = iter(testloader)\n",
    "inputs, labels, indeces = data_iter.next()\n",
    "inputs = inputs.to(device).float()\n",
    "inputs, predicted_classes = _get_input_subset(model_robust, inputs, subset_size, decision_threshold=0)\n",
    "print(inputs.shape)\n",
    "print(inputs[0].reshape(1, -1))\n",
    "\n",
    "cfe_cchvae = []\n",
    "for i in range(len(inputs)):\n",
    "    if i%10 == 0:\n",
    "        print(\"Gotten {} done...\".format(i))\n",
    "    cfe, ctr_val = cchvae.generate_counterfactuals(\n",
    "                    query_instance= inputs[i].reshape(1, -1),\n",
    "                    target_class= 1 )\n",
    "    if ctr_val.item() > 0:\n",
    "        cfe_cchvae.append(cfe)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3903cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d8e640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e654d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfdc672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20211549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7a4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1321b258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gotten 0 done...\n",
      "Gotten 100 done...\n",
      "Gotten 200 done...\n",
      "Gotten 300 done...\n",
      "Gotten 400 done...\n",
      "Gotten 500 done...\n",
      "Gotten 600 done...\n",
      "Gotten 700 done...\n",
      "Gotten 800 done...\n",
      "Gotten 900 done...\n",
      "Gotten 1000 done...\n",
      "Gotten 1100 done...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "subset_size = 5\n",
    "testloader = DataLoader(dataset_test,\n",
    "                        batch_size=data_meta_info[\"n_starting_instances\"],\n",
    "                        shuffle=False)\n",
    "\n",
    "data_iter = iter(testloader)\n",
    "inputs, labels, indeces = data_iter.next()\n",
    "inputs = inputs.to(device).float()\n",
    "inputs, predicted_classes = _get_input_subset(model_robust, inputs, subset_size, decision_threshold=0)\n",
    "\n",
    "\n",
    "cfe_cchvae = []\n",
    "for i in range(len(inputs)):\n",
    "    if i%100 == 0:\n",
    "        print(\"Gotten {} done...\".format(i))\n",
    "    ctr_val = scfe_r.generate_counterfactuals(\n",
    "                    query_instance=inputs[i],\n",
    "                    target_class=1)\n",
    "    if ctr_val > 0:\n",
    "        distance_scfe_r.append(ctr_val)\n",
    "        \n",
    "# distance_scfe_r = [scfe_r.generate_counterfactuals(\n",
    "#                     query_instance=inputs[i],\n",
    "#                     target_class=1).item() for i in range(len(inputs)) if scfe_r.generate_counterfactuals(\n",
    "#                     query_instance=inputs[i],\n",
    "#                     target_class=1) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "11d2020d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distance_scfe_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d8b021e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = [\"base\"]*len(distance_scfe) + [\"robust\"]*len(distance_scfe_r) # + len(distance_scfe))\n",
    "cost_vals = [i.item() for i in distance_scfe] + [i.item() for i in distance_scfe_r]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "10c94cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'base',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " 'robust',\n",
       " ...]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "671c2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "scfe_data_df = pd.DataFrame(list(zip(cost_vals, model_name)), columns=[\"Cost\", \"Model\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4b3654c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scfe_data_df.to_csv(\"{}_scfe_data_df.csv\".format(data_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2e0d1de9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [113]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdistance_scfe\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdistance_scfe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "sum(distance_scfe)/len(distance_scfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f63e8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validity : \n",
    "\n",
    "# Step 1 : Get counter-factuals for baseline\n",
    "# Step 2 : Apply to robust models to see if the prediction is 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a7897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
